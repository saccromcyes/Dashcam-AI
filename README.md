# Dashcam AI
Screen Capture: Continuously take screenshots or video frames of the desktop. For example, in Python, you can use PyAutoGUI to grab the screen (pyautogui.screenshot() returns a PIL image). On a 1920×1080 display, this is ~100 ms per full screenshot. You can capture either a full screen or a region. These frames serve as the visual input for understanding UI events.

Audio Capture: Record microphone input in real time (e.g., with sounddevice or PyAudio). One can buffer short audio clips (e.g., 1–2 seconds) or record continuously. These snippets will be transcribed to text so spoken commands like “open Excel” are understood.

Speech-to-Text: Transcribe captured audio locally. Use an offline ASR model such as OpenAI’s Whisper (open-source, self-hostable) or Vosk

. For example, Whisper’s models (released 2022) can run on-device for decent accuracy

. Vosk provides small (~50 MB) language models for many languages and runs with near-zero latency

. The result is a text transcript of any voice commands.

Image-to-Text (OCR): To interpret what’s on screen (menus, text fields, dialog titles, etc.), apply OCR to screenshots. For example, use Tesseract via pytesseract – “a Python wrapper for Google’s Tesseract-OCR engine”.Each frame is fed to Tesseract to extract on-screen text. This way, the system “reads” labels, file names, form fields, etc. For GUI buttons without text, image recognition (e.g., template matching in PyAutoGUI) can detect UI elements too.

Event Logging: Track user actions (mouse moves, clicks, keystrokes). You can instrument this by intercepting OS input events (e.g. using hooks) or by replaying: PyAutoGUI can record/replay macros, or you can write listeners that log coordinates and keys. Each action is timestamped and stored. Ultimately, build a structured event log (JSON) of the session. This log captures what the user did. Screenshots or screen video clips can also be archived alongside for reference, though only summaries are needed long-term.

Pattern Recognition: Over time, analyze the logs and transcripts to find repeated sequences. For instance, if every day the user opens Excel, enters data, and saves, that sequence is a candidate for automation. You can use simple heuristics or clustering (e.g., look for identical sequences of actions) to detect loops. The AI “notices” that certain tasks recur.

LLM Understanding: Feed the structured logs (and optionally the transcripts/screenshots) into a local language model to generate a human-readable description of the workflow. For example, a model like Mistral Small or an offline LLaMA model (via LLaMA.cpp) can interpret the events and output text

Feedback & Suggestions: The system can report detected patterns to the user, e.g.: “Detected repetitive workflow: Opening Excel → entering values → saving. Would you like to automate this sequence?” This plain-text suggestion is generated by the LLM based on the logs. Here, we essentially implement the “suggest possible automations” step of the hackathon prompt.

